{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84681812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "import os\n",
    "import gc\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d679950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_filters,hidden_size,filter_size=3,num_classes=10,activation_function='ReLU',n_blocks=5,filter_organisation_factor=2,dropout=0.4,batch_norm=True):\n",
    "        \"\"\"\n",
    "        Custom CNN model for image classification.\n",
    "        Args:\n",
    "            num_filters (int): Number of filters in the first convolutional layer.\n",
    "            size_filter (int): Size of the convolutional filters.\n",
    "            num_classes (int): Number of output classes.\n",
    "            activation (str): Activation function to use .\n",
    "            n_blocks (int): Number of convolutional blocks.\n",
    "        \"\"\"\n",
    "        super(CustomCNN,self).__init__()\n",
    "        self.conv = nn.ModuleList()\n",
    "        self.activation = {\"ReLU\": nn.ReLU(), \"GeLU\": nn.GELU(), \"SiLU\": nn.SiLU(), \"Mish\": nn.Mish()}\n",
    "        in_channel = 3\n",
    "        # convolutional blocks\n",
    "        for i in range(n_blocks):\n",
    "            in_channel = num_filters*(filter_organisation_factor**(i-1)) if i>0 else 3\n",
    "            in_channel = int(in_channel)\n",
    "            numberOfFilters = num_filters*(filter_organisation_factor**(i))\n",
    "            numberOfFilters = int(numberOfFilters)\n",
    "            # start of the convolutional block\n",
    "            # convolution layer\n",
    "            self.conv.append(nn.Conv2d(in_channels = in_channel, out_channels=numberOfFilters, kernel_size=filter_size, stride=1))\n",
    "            # batch normalization layer\n",
    "            if batch_norm:\n",
    "                self.conv.append(nn.BatchNorm2d(numberOfFilters))\n",
    "            # activation layer\n",
    "            self.conv.append(self.activation[activation_function])\n",
    "            # max pooling layer\n",
    "            self.conv.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # flattening last layer\n",
    "        self.conv.append(nn.AdaptiveAvgPool2d(1))\n",
    "        self.conv.append(nn.Flatten())\n",
    "        # fully connected layer\n",
    "        in_channel = num_filters*(filter_organisation_factor**(n_blocks-1))\n",
    "        in_channel = int(in_channel)    \n",
    "        self.conv.append(nn.Linear(in_channel, hidden_size))\n",
    "        self.conv.append(self.activation['GeLU'])\n",
    "        # dropout layer\n",
    "        self.conv.append(nn.Dropout(dropout))\n",
    "        self.conv.append(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.conv:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def train_model(self, \n",
    "                   train_loader: DataLoader, \n",
    "                   val_loader: DataLoader, \n",
    "                   epochs: int, \n",
    "                   learning_rate: float, \n",
    "                   device: torch.device,\n",
    "                   test_loader: DataLoader = None,\n",
    "                   criterion: nn.Module = nn.CrossEntropyLoss(),\n",
    "                   optimizer_class: optim.Optimizer = optim.Adam):\n",
    "        \"\"\"\n",
    "        Train the model with accuracy evaluation.\n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader for training data.\n",
    "            val_loader (DataLoader): DataLoader for validation data.\n",
    "            epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for optimizer.\n",
    "            device (torch.device): Device to train on ('cuda' or 'cpu').\n",
    "            criterion (nn.Module): Loss function.\n",
    "            optimizer_class (optim.Optimizer): Optimizer class (e.g., Adam, SGD).\n",
    "        Returns:\n",
    "            Dict[str, List[float]]: Dictionary containing training/validation losses and accuracies.\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        optimizer = optimizer_class(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_acc':[]\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_correct += (outputs.argmax(1) == labels).sum().item()\n",
    " \n",
    "                del loss\n",
    "                del inputs\n",
    "            # Calculate training metrics\n",
    "            epoch_train_loss = running_loss / total_samples\n",
    "            epoch_train_acc = running_correct / total_samples\n",
    "            history['train_loss'].append(epoch_train_loss)\n",
    "            history['train_acc'].append(epoch_train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    batch_size = inputs.size(0)\n",
    "                    val_total += batch_size\n",
    "                    \n",
    "                    outputs = self(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item() * batch_size\n",
    "                    val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                    del loss\n",
    "                    del inputs\n",
    "            if test_loader is not None:\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        batch_size = inputs.size(0)\n",
    "                        test_total += batch_size\n",
    "                        \n",
    "                        outputs = self(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                        del loss\n",
    "                        del inputs\n",
    "            # Calculate validation metrics\n",
    "            epoch_val_loss = val_loss / val_total\n",
    "            epoch_val_acc = val_correct / val_total\n",
    "            epoch_test_acc = test_correct / test_total if test_loader is not None else None\n",
    "            history['val_loss'].append(epoch_val_loss)\n",
    "            history['val_acc'].append(epoch_val_acc)\n",
    "            history['test_acc'].append(epoch_test_acc)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f'Epoch {epoch+1}/{epochs}: '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    " \n",
    "        return history\n",
    "    \n",
    "    def predict(self, \n",
    "                test_loader: DataLoader, \n",
    "                device: torch.device) :\n",
    "        \"\"\"\n",
    "        Make predictions on test data.\n",
    "        Args:\n",
    "            test_loader (DataLoader): DataLoader for test data.\n",
    "            device (torch.device): Device to use for prediction.\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Predictions and true labels.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = self(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "        \n",
    "        return torch.cat(all_preds), torch.cat(all_labels)\n",
    "\n",
    "def transform_image(dataAugmentation=False):\n",
    "    if dataAugmentation:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        return transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def data_loader(data_dir, batch_size, dataAugmentation, num_workers=3):\n",
    "    # Load the full training dataset\n",
    "    full_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform_image(dataAugmentation=dataAugmentation))\n",
    "    targets = full_dataset.targets  # class labels for stratification\n",
    "\n",
    "    # Stratified split: 80% train, 20% val\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(full_dataset.samples, targets))\n",
    "\n",
    "    # Subsets\n",
    "    train_dataset = Subset(full_dataset, train_idx)\n",
    "    val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "    # Override transform for val set (no augmentation)\n",
    "    val_dataset.dataset.transform = transform_image(dataAugmentation=False)\n",
    "\n",
    "    # Test dataset\n",
    "    test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=transform_image(dataAugmentation=False))\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061ab4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Assuming CustomCNN, stratified_split, and transform_image are already defined\n",
    "\n",
    "train_loader, val_loader, test_loader = data_loader(data_dir='data', batch_size=32, dataAugmentation=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = CustomCNN(num_filters=64,\n",
    "#                     hidden_size=128,\n",
    "#                     filter_organisation_factor=2,\n",
    "#                     dropout=0.4,\n",
    "#                     batch_norm=True,\n",
    "#                     n_blocks=5,\n",
    "#                     activation_function=\"ReLU\").to(device)\n",
    "\n",
    "# history = model.train_model(train_loader=train_loader,\n",
    "#                             val_loader=val_loader,\n",
    "#                             test_loader=test_loader,\n",
    "#                             epochs=2,\n",
    "#                             learning_rate=0.001,\n",
    "#                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f82b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum([p.numel() for p in model.parameters()])*1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b3358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    wandb.run.name = (\n",
    "        f\"n_filters_{config.num_filters}_act_{config.activation_function}_\"\n",
    "        f\"fof_{config.filter_organisation_factor}_dropout_{config.dropout}_\"\n",
    "        f\"bn_{config.batch_norm}_data_aug_{config.data_aug}_hs_{config.hidden_size}_\"\n",
    "        f\"n_blocks_{config.n_blocks}_num_epochs_{config.epochs}\"\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CustomCNN(num_filters=config.num_filters,\n",
    "                      hidden_size=config.hidden_size,\n",
    "                      filter_organisation_factor=config.filter_organisation_factor,\n",
    "                      dropout=config.dropout,\n",
    "                      batch_norm=config.batch_norm,\n",
    "                      n_blocks=config.n_blocks,\n",
    "                      activation_function=config.activation_function).to(device)\n",
    "    \n",
    "    history = model.train_model(train_loader=train_loader,\n",
    "                                val_loader=val_loader,\n",
    "                                test_loader=test_loader,\n",
    "                                epochs=config.epochs,\n",
    "                                learning_rate=0.001,\n",
    "                                device=device)\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    for epoch in range(len(history['train_loss'])):\n",
    "        wandb.log({\n",
    "            'train_loss': history['train_loss'][epoch],\n",
    "            'val_loss': history['val_loss'][epoch],\n",
    "            'train_acc': history['train_acc'][epoch],\n",
    "            'val_acc': history['val_acc'][epoch],\n",
    "            'test_acc': history['test_acc'][epoch],\n",
    "            'epoch': epoch\n",
    "        })\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3c3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: enhlczy2\n",
      "Sweep URL: https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/sweeps/enhlczy2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: 93rj26b8 with config:\n",
      "wandb: \tactivation_function: GeLU\n",
      "wandb: \tbatch_norm: True\n",
      "wandb: \tdata_aug: True\n",
      "wandb: \tdropout: 0.4\n",
      "wandb: \tepochs: 15\n",
      "wandb: \tfilter_organisation_factor: 1.5\n",
      "wandb: \thidden_size: 512\n",
      "wandb: \tn_blocks: 5\n",
      "wandb: \tnum_filters: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\code\\DL\\da6401_assignment2\\wandb\\run-20250416_195353-93rj26b8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/runs/93rj26b8' target=\"_blank\">hopeful-sweep-1</a></strong> to <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/sweeps/enhlczy2' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/sweeps/enhlczy2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/sweeps/enhlczy2' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/sweeps/enhlczy2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/runs/93rj26b8' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/runs/93rj26b8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: Train Loss: 2.1346 | Train Acc: 0.2213 | Val Loss: 2.1473 | Val Acc: 0.2215\n",
      "Epoch 2/15: Train Loss: 2.0252 | Train Acc: 0.2690 | Val Loss: 2.0943 | Val Acc: 0.2615\n",
      "Epoch 3/15: Train Loss: 1.9857 | Train Acc: 0.2830 | Val Loss: 1.9668 | Val Acc: 0.2915\n",
      "Epoch 4/15: Train Loss: 1.9360 | Train Acc: 0.3078 | Val Loss: 1.9865 | Val Acc: 0.2945\n",
      "Epoch 5/15: Train Loss: 1.8952 | Train Acc: 0.3173 | Val Loss: 1.9975 | Val Acc: 0.2985\n",
      "Epoch 6/15: Train Loss: 1.8616 | Train Acc: 0.3345 | Val Loss: 1.9263 | Val Acc: 0.3315\n",
      "Epoch 7/15: Train Loss: 1.8307 | Train Acc: 0.3482 | Val Loss: 1.8486 | Val Acc: 0.3580\n",
      "Epoch 8/15: Train Loss: 1.7909 | Train Acc: 0.3588 | Val Loss: 1.8374 | Val Acc: 0.3720\n",
      "Epoch 9/15: Train Loss: 1.7555 | Train Acc: 0.3754 | Val Loss: 1.8551 | Val Acc: 0.3715\n",
      "Epoch 10/15: Train Loss: 1.7287 | Train Acc: 0.3898 | Val Loss: 1.7895 | Val Acc: 0.3730\n",
      "Epoch 11/15: Train Loss: 1.7003 | Train Acc: 0.3973 | Val Loss: 1.7433 | Val Acc: 0.4025\n",
      "Epoch 12/15: Train Loss: 1.6772 | Train Acc: 0.4019 | Val Loss: 1.7627 | Val Acc: 0.3835\n",
      "Epoch 13/15: Train Loss: 1.6401 | Train Acc: 0.4263 | Val Loss: 1.7440 | Val Acc: 0.3915\n",
      "Epoch 14/15: Train Loss: 1.6152 | Train Acc: 0.4308 | Val Loss: 1.7394 | Val Acc: 0.4065\n",
      "Epoch 15/15: Train Loss: 1.5872 | Train Acc: 0.4403 | Val Loss: 1.7339 | Val Acc: 0.4010\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>test_acc</td><td>▁▂▄▃▄▅▆▇▆▇█▇▇██</td></tr><tr><td>train_acc</td><td>▁▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▅▄▄▃▃▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▄▄▅▆▇▇▇█▇▇██</td></tr><tr><td>val_loss</td><td>█▇▅▅▅▄▃▃▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>14</td></tr><tr><td>test_acc</td><td>0.392</td></tr><tr><td>train_acc</td><td>0.44031</td></tr><tr><td>train_loss</td><td>1.58716</td></tr><tr><td>val_acc</td><td>0.401</td></tr><tr><td>val_loss</td><td>1.73386</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">n_filters_16_act_GeLU_fof_1.5_dropout_0.4_bn_True_data_aug_True_hs_512_n_blocks_5_num_epochs_15</strong> at: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/runs/93rj26b8' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/runs/93rj26b8</a><br> View project at: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250416_195353-93rj26b8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  \n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_filters\": {\"values\": [16]},\n",
    "        \"activation_function\": {\"values\": [\"GeLU\"]},\n",
    "        \"filter_organisation_factor\": {\"values\": [1.5]},\n",
    "        \"dropout\": {\"values\": [0.4]},\n",
    "        \"batch_norm\": {\"values\": [True]},\n",
    "        \"data_aug\": {\"values\": [True]},\n",
    "        \"hidden_size\": {\"values\": [512]},\n",
    "        \"n_blocks\": {\"values\": [5]},\n",
    "        \"epochs\": {\"values\": [15]},\n",
    "    }\n",
    "}\n",
    "\n",
    "project_name = \"iNaturalist-CNN-Optimization\"\n",
    "\n",
    "# Create sweep\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep_config,\n",
    "    project=\"iNaturalist-CNN-Optimization\",\n",
    ")\n",
    "\n",
    "# Run agent for N trials\n",
    "# wandb.agent(sweep_id=sweep_id, function=train, count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039f1cd",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba36f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Load Dataset\n",
    "# ----------------------------\n",
    "\n",
    "train_loader, val_loader, test_loader = data_loader(data_dir='data', batch_size=32, dataAugmentation=True)\n",
    "# ----------------------------\n",
    "# Load Pretrained Model\n",
    "# ----------------------------\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Strategy: Freeze all layers except final fc\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer for 1010-class classification\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ----------------------------\n",
    "# Loss and Optimizer\n",
    "# ----------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LR)\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = validate(model, val_loader)\n",
    "    test_acc = validate(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Accuracy: {val_acc:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Optional: Save Model\n",
    "torch.save(model.state_dict(), \"resnet50_inaturalist_finetuned.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
