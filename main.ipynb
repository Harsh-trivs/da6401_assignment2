{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84681812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "import os\n",
    "import gc\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d679950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_filters,hidden_size,filter_size=3,num_classes=10,activation_function='ReLU',n_blocks=5,filter_organisation_factor=2,dropout=0.4,batch_norm=True):\n",
    "        \"\"\"\n",
    "        Custom CNN model for image classification.\n",
    "        Args:\n",
    "            num_filters (int): Number of filters in the first convolutional layer.\n",
    "            size_filter (int): Size of the convolutional filters.\n",
    "            num_classes (int): Number of output classes.\n",
    "            activation (str): Activation function to use .\n",
    "            n_blocks (int): Number of convolutional blocks.\n",
    "        \"\"\"\n",
    "        super(CustomCNN,self).__init__()\n",
    "        self.conv = nn.ModuleList()\n",
    "        self.activation = {\"ReLU\": nn.ReLU(), \"GeLU\": nn.GELU(), \"SiLU\": nn.SiLU(), \"Mish\": nn.Mish()}\n",
    "        in_channel = 3\n",
    "        # convolutional blocks\n",
    "        for i in range(n_blocks):\n",
    "            in_channel = num_filters*(filter_organisation_factor**(i-1)) if i>0 else 3\n",
    "            in_channel = int(in_channel)\n",
    "            numberOfFilters = num_filters*(filter_organisation_factor**(i))\n",
    "            numberOfFilters = int(numberOfFilters)\n",
    "            # start of the convolutional block\n",
    "            # convolution layer\n",
    "            self.conv.append(nn.Conv2d(in_channels = in_channel, out_channels=numberOfFilters, kernel_size=filter_size, stride=1))\n",
    "            # batch normalization layer\n",
    "            if batch_norm:\n",
    "                self.conv.append(nn.BatchNorm2d(numberOfFilters))\n",
    "            # activation layer\n",
    "            self.conv.append(self.activation[activation_function])\n",
    "            # max pooling layer\n",
    "            self.conv.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # flattening last layer\n",
    "        self.conv.append(nn.AdaptiveAvgPool2d(1))\n",
    "        self.conv.append(nn.Flatten())\n",
    "        # fully connected layer\n",
    "        in_channel = num_filters*(filter_organisation_factor**(n_blocks-1))\n",
    "        in_channel = int(in_channel)    \n",
    "        self.conv.append(nn.Linear(in_channel, hidden_size))\n",
    "        self.conv.append(self.activation['GeLU'])\n",
    "        # dropout layer\n",
    "        self.conv.append(nn.Dropout(dropout))\n",
    "        self.conv.append(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.conv:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def train_model(self, \n",
    "                   train_loader: DataLoader, \n",
    "                   val_loader: DataLoader, \n",
    "                   epochs: int, \n",
    "                   learning_rate: float, \n",
    "                   device: torch.device,\n",
    "                   criterion: nn.Module = nn.CrossEntropyLoss(),\n",
    "                   optimizer_class: optim.Optimizer = optim.Adam):\n",
    "        \"\"\"\n",
    "        Train the model with accuracy evaluation.\n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader for training data.\n",
    "            val_loader (DataLoader): DataLoader for validation data.\n",
    "            epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for optimizer.\n",
    "            device (torch.device): Device to train on ('cuda' or 'cpu').\n",
    "            criterion (nn.Module): Loss function.\n",
    "            optimizer_class (optim.Optimizer): Optimizer class (e.g., Adam, SGD).\n",
    "        Returns:\n",
    "            Dict[str, List[float]]: Dictionary containing training/validation losses and accuracies.\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        optimizer = optimizer_class(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_correct += (outputs.argmax(1) == labels).sum().item()\n",
    " \n",
    "                del loss\n",
    "                del inputs\n",
    "            # Calculate training metrics\n",
    "            epoch_train_loss = running_loss / total_samples\n",
    "            epoch_train_acc = running_correct / total_samples\n",
    "            history['train_loss'].append(epoch_train_loss)\n",
    "            history['train_acc'].append(epoch_train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    batch_size = inputs.size(0)\n",
    "                    val_total += batch_size\n",
    "                    \n",
    "                    outputs = self(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item() * batch_size\n",
    "                    val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                    del loss\n",
    "                    del inputs\n",
    "            # Calculate validation metrics\n",
    "            epoch_val_loss = val_loss / val_total\n",
    "            epoch_val_acc = val_correct / val_total\n",
    "            history['val_loss'].append(epoch_val_loss)\n",
    "            history['val_acc'].append(epoch_val_acc)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f'Epoch {epoch+1}/{epochs}: '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    " \n",
    "        return history\n",
    "    \n",
    "    def predict(self, \n",
    "                test_loader: DataLoader, \n",
    "                device: torch.device) :\n",
    "        \"\"\"\n",
    "        Make predictions on test data.\n",
    "        Args:\n",
    "            test_loader (DataLoader): DataLoader for test data.\n",
    "            device (torch.device): Device to use for prediction.\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Predictions and true labels.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = self(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "        \n",
    "        return torch.cat(all_preds), torch.cat(all_labels)\n",
    "\n",
    "def transform_image(dataAugmentation=False):\n",
    "    if dataAugmentation:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],       \n",
    "                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],       \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def data_loader(data_dir, batch_size, dataAugmentation, num_workers=3):\n",
    "    # Load the full training dataset\n",
    "    full_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform_image(dataAugmentation=dataAugmentation))\n",
    "    targets = full_dataset.targets  # class labels for stratification\n",
    "\n",
    "    # Stratified split: 80% train, 20% val\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(full_dataset.samples, targets))\n",
    "\n",
    "    # Subsets\n",
    "    train_dataset = Subset(full_dataset, train_idx)\n",
    "    val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "    # Override transform for val set (no augmentation)\n",
    "    val_dataset.dataset.transform = transform_image(dataAugmentation=False)\n",
    "\n",
    "    # Test dataset\n",
    "    test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=transform_image(dataAugmentation=False))\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b3358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8uhfd32b\n",
      "Sweep URL: https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN-Optimization/sweeps/8uhfd32b\n"
     ]
    }
   ],
   "source": [
    "from partA.model import CustomCNN, data_loader\n",
    "import torch\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "def train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "    train_loader, val_loader, test_loader = data_loader(data_dir='data', batch_size=32, dataAugmentation=config.data_aug)\n",
    "    wandb.run.name = (\n",
    "        f\"n_filters_{config.num_filters}_act_{config.activation_function}_\"\n",
    "        f\"fof_{config.filter_organisation_factor}_dropout_{config.dropout}_\"\n",
    "        f\"bn_{config.batch_norm}_data_aug_{config.data_aug}_hs_{config.hidden_size}_\"\n",
    "        f\"n_blocks_{config.n_blocks}_num_epochs_{config.epochs}\"\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CustomCNN(num_filters=config.num_filters,\n",
    "                      hidden_size=config.hidden_size,\n",
    "                      filter_organisation_factor=config.filter_organisation_factor,\n",
    "                      dropout=config.dropout,\n",
    "                      batch_norm=config.batch_norm,\n",
    "                      n_blocks=config.n_blocks,\n",
    "                      activation_function=config.activation_function).to(device)\n",
    "    \n",
    "    history = model.train_model(train_loader=train_loader,\n",
    "                                val_loader=val_loader,\n",
    "                                epochs=config.epochs,\n",
    "                                learning_rate=0.001,\n",
    "                                device=device)\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    for epoch in range(len(history['train_loss'])):\n",
    "        wandb.log({\n",
    "            'train_loss': history['train_loss'][epoch],\n",
    "            'val_loss': history['val_loss'][epoch],\n",
    "            'train_acc': history['train_acc'][epoch],\n",
    "            'val_acc': history['val_acc'][epoch],\n",
    "            'epoch': epoch\n",
    "        })\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    del model\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"activation_function\": {\"values\": [\"SiLU\", \"Mish\", \"GeLU\"]},\n",
    "        \"batch_norm\": {\"values\": [True, False]},\n",
    "        \"data_aug\": {\"values\": [True, False]},\n",
    "        \"dropout\": {\"values\": [0.2, 0.3, 0.4]},\n",
    "        \"epochs\": {\"values\": [5, 10, 15]},\n",
    "        \"filter_organisation_factor\": {\"values\": [0.5, 1, 1.5]},\n",
    "        \"hidden_size\": {\"values\": [128, 256]},\n",
    "        \"num_filters\": {\"values\": [16, 32, 64]},\n",
    "        \"n_blocks\": {\"values\": [5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "project_name = \"iNaturalist-CNN-Optimization\"\n",
    "\n",
    "# Create sweep\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep_config,\n",
    "    project=\"iNaturalist-CNN-Optimization\",\n",
    ")\n",
    "\n",
    "# Run agent for N trials\n",
    "# wandb.agent(sweep_id=sweep_id, function=train, count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc38cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model found. Skipping training...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">best_model_test_eval</strong> at: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN/runs/er16ygpi' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN/runs/er16ygpi</a><br> View project at: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250419_183622-er16ygpi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\code\\DL\\da6401_assignment2\\wandb\\run-20250419_183728-gpjkap2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN/runs/gpjkap2f' target=\"_blank\">best_model_test_eval</a></strong> to <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN/runs/gpjkap2f' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/iNaturalist-CNN/runs/gpjkap2f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from partA.model import CustomCNN, data_loader\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "\n",
    "train_loader, val_loader, test_loader = data_loader(data_dir='data', batch_size=32, dataAugmentation=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomCNN(\n",
    "                    filter_size=3,\n",
    "                    num_classes=10,\n",
    "                    num_filters=64,\n",
    "                    hidden_size=256,\n",
    "                    filter_organisation_factor=1.5,\n",
    "                    dropout=0.4,\n",
    "                    batch_norm=True,\n",
    "                    n_blocks=5,\n",
    "                    activation_function=\"GeLU\").to(device)\n",
    "\n",
    "# Check if the best model exists\n",
    "if os.path.exists(\"partA/best_model.pth\"):\n",
    "    print(\"Best model found. Skipping training...\")\n",
    "    model.load_state_dict(torch.load(\"partA/best_model.pth\"))\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"No best model found. Starting training...\")\n",
    "    history = model.train_model(train_loader=train_loader,\n",
    "                                val_loader=val_loader,\n",
    "                                epochs=15,\n",
    "                                learning_rate=0.001,\n",
    "                                device=device)\n",
    "    # Save the best model\n",
    "    torch.save(model.state_dict(), \"partA/best_model.pth\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"partA/best_model.pth\"))\n",
    "# ---- Get best epoch based on test accuracy ----\n",
    "predictions,true_labels = model.predict(test_loader, device)\n",
    "best_accuracy = (predictions == true_labels).sum().item() / len(true_labels)\n",
    "\n",
    "dataset = datasets.ImageFolder(root=os.path.join('data', 'val'))\n",
    "class_to_name = {i: name.split('/')[-1] for i, name in enumerate(dataset.classes)}\n",
    "                               \n",
    "\n",
    "# ---- Initialize Weights & Biases ----\n",
    "wandb.init(project=\"iNaturalist-CNN\", name=\"best_model_test_eval\")\n",
    "\n",
    "# ---- Get 30 sample predictions ----\n",
    "model.eval()\n",
    "images, labels, preds = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        images.extend(inputs.cpu())\n",
    "        labels.extend(targets.cpu())\n",
    "        preds.extend(predicted.cpu())\n",
    "\n",
    "        if len(images) >= 30:\n",
    "            break\n",
    "\n",
    "images = images[:30]\n",
    "labels = labels[:30]\n",
    "preds = preds[:30]\n",
    "\n",
    "# ---- Create 10×3 prediction grid ----\n",
    "wandb_images =[]\n",
    "for i in range(30):\n",
    "        # Denormalize image\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Create caption with prediction info\n",
    "        caption = (f\"Correct ✅\" if labels[i] == preds[i] else f\"Wrong ❌\") + \\\n",
    "                 f\"\\nPredicted: {class_to_name[labels[i].item()]}\\nActual: {class_to_name[preds[i].item()]}\"\n",
    "        \n",
    "        # Create wandb.Image with caption\n",
    "        wandb_images.append(wandb.Image(\n",
    "            img, \n",
    "            caption=caption,\n",
    "        ))\n",
    "\n",
    "# ---- Log grid and accuracy to wandb ----\n",
    "wandb.log({\n",
    "    \"Prediction Grid\": wandb_images,\n",
    "    \"Best Test Accuracy\": best_accuracy\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039f1cd",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba36f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: harshtrivs (harshtrivs-indian-institute-of-technology-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\code\\DL\\da6401_assignment2\\wandb\\run-20250419_031141-hju28as0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist/runs/hju28as0' target=\"_blank\">logical-wood-1</a></strong> to <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist/runs/hju28as0' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist/runs/hju28as0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:07<00:00,  3.73it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:23<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1836 | Train Acc: 0.6116 | Val Acc: 0.6945 | Test Acc: 0.6945\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:04<00:00,  3.89it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.66it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7509 | Train Acc: 0.7573 | Val Acc: 0.7255 | Test Acc: 0.7255\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:03<00:00,  3.91it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.66it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5020 | Train Acc: 0.8392 | Val Acc: 0.7040 | Test Acc: 0.7150\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:03<00:00,  3.91it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.64it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3540 | Train Acc: 0.8862 | Val Acc: 0.7190 | Test Acc: 0.7210\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:04<00:00,  3.89it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.69it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2502 | Train Acc: 0.9185 | Val Acc: 0.7020 | Test Acc: 0.7215\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:04<00:00,  3.90it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:23<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0969 | Train Acc: 0.9704 | Val Acc: 0.7710 | Test Acc: 0.7755\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:04<00:00,  3.88it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:24<00:00,  2.62it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:23<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0419 | Train Acc: 0.9891 | Val Acc: 0.7770 | Test Acc: 0.7685\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:03<00:00,  3.92it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0274 | Train Acc: 0.9940 | Val Acc: 0.7755 | Test Acc: 0.7750\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:03<00:00,  3.92it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.67it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:21<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0194 | Train Acc: 0.9955 | Val Acc: 0.7785 | Test Acc: 0.7770\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:03<00:00,  3.93it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.64it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0162 | Train Acc: 0.9951 | Val Acc: 0.7775 | Test Acc: 0.7730\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:04<00:00,  3.89it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:25<00:00,  2.47it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0110 | Train Acc: 0.9977 | Val Acc: 0.7775 | Test Acc: 0.7760\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:04<00:00,  3.90it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.65it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0104 | Train Acc: 0.9981 | Val Acc: 0.7730 | Test Acc: 0.7735\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:03<00:00,  3.92it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.66it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0109 | Train Acc: 0.9979 | Val Acc: 0.7810 | Test Acc: 0.7750\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:03<00:00,  3.93it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.72it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0087 | Train Acc: 0.9987 | Val Acc: 0.7800 | Test Acc: 0.7785\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [01:04<00:00,  3.90it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:23<00:00,  2.72it/s]\n",
      "Testing: 100%|██████████| 63/63 [00:22<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0078 | Train Acc: 0.9986 | Val Acc: 0.7770 | Test Acc: 0.7765\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>lr</td><td>█████▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▄▃▃▃█▇████████</td></tr><tr><td>train_acc</td><td>▁▄▅▆▇▇█████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▂▃▂▇█████▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>test_acc</td><td>0.7765</td></tr><tr><td>train_acc</td><td>0.99862</td></tr><tr><td>train_loss</td><td>0.00777</td></tr><tr><td>val_acc</td><td>0.777</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-wood-1</strong> at: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist/runs/hju28as0' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist/runs/hju28as0</a><br> View project at: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/resnet50-finetune-inaturalist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250419_031141-hju28as0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model import data_loader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 15\n",
    "LR = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Init wandb\n",
    "# ----------------------------\n",
    "wandb.init(project=\"resnet50-finetune-inaturalist\", config={\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"lr\": LR,\n",
    "    \"architecture\": \"resnet50\",\n",
    "    \"unfrozen_layers\": \"layer4 + fc\",\n",
    "})\n",
    "\n",
    "# ----------------------------\n",
    "# Load Dataset\n",
    "# ----------------------------\n",
    "train_loader, val_loader, test_loader = data_loader(\n",
    "    data_dir='data', batch_size=BATCH_SIZE, dataAugmentation=True\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Load Pretrained Model\n",
    "# ----------------------------\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last conv block (layer4)\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace final layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ----------------------------\n",
    "# Loss, Optimizer, Scheduler\n",
    "# ----------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=LR\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# ----------------------------\n",
    "# Training & Evaluation\n",
    "# ----------------------------\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(output, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    acc = correct / total\n",
    "    return running_loss / len(loader), acc\n",
    "\n",
    "\n",
    "def validate(model, loader, desc=\"Validation\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=desc):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# ----------------------------\n",
    "# Main Training Loop\n",
    "# ----------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = validate(model, val_loader, desc=\"Validation\")\n",
    "    test_acc = validate(model, test_loader, desc=\"Testing\")\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"lr\": scheduler.get_last_lr()[0]\n",
    "    })\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# ----------------------------\n",
    "# Save model\n",
    "# ----------------------------\n",
    "torch.save(model.state_dict(), \"resnet50_inaturalist_finetuned.pth\")\n",
    "wandb.save(\"resnet50_inaturalist_finetuned.pth\")\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
